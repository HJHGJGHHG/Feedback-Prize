{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Simple PyTorch NER Training on GPU\n",
    "\n",
    "#### A simple way to train an NER model that will classify each token into a discourse type. This uses the Hugging Face `Trainer` and their default `ModelForTokenClassification`, but you can also stick a custom `nn.Module` into it as well. It has already been seen that Longformer does well, so let's try its close relative: BigBird. It's another model that has a novel attention mechanism that can handle long sequence lengths. There are some benchmarks for long range models that show BigBird doing better than Longformer (see image below), but we'll have to see what works better for this competition. This is using the base version of BigBird, but you'll probably get better performance on the large version -- it will just take longer to train. \n",
    "\n",
    "#### This is only 1 fold and includes macro-f1 CV score of 0.535. It would be wise to do k-fold validation with your favorite value of k to get a better sense of how your model is doing.\n",
    "\n",
    "## Coming soon:\n",
    "### âœ… Custom evaluate function that does competition-specific f1 scoring\n",
    "### ðŸ”³ Model with custom output layers\n",
    "### ðŸ”³ Model that has been pretrained further on in-domain data\n",
    "### ðŸ”³ Comparison of BIO vs IO labeling scheme\n",
    "### ðŸ”³ Any suggestions?\n",
    "\n",
    "\n",
    "### ðŸš¨ It appears that there might be an issue with the bigbird tokenizer ([warning similar to here](https://github.com/huggingface/transformers/pull/11075#issuecomment-833404825)). Will update this when confirming it is/isn't working properly.\n",
    "\n",
    "\n",
    "### ðŸ‘‰ Version 5 has the outputs of the most recent full training\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/1694368/102184635-baab7400-3eea-11eb-8113-b3fb6d4b8bbc.png\" alt=\"benchmark comparison\" width=\"800\"/>\n",
    "\n",
    "### Code based off of: \n",
    "\n",
    "- [run_ner.py](https://github.com/huggingface/transformers/blob/master/examples/pytorch/token-classification/run_ner.py)\n",
    "- [Rob Mulla's implementation of f1 score (@robikscube)](https://www.kaggle.com/robikscube/student-writing-competition-twitch)\n",
    "- [zzy's infer notebook](https://www.kaggle.com/zzy990106/pytorch-ner-infer?scriptVersionId=82677278&cellId=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary for metrics\n",
    "!pip install seqeval -qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datasets\n",
    "from datasets import ClassLabel, load_dataset, load_metric, Dataset\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForTokenClassification,\n",
    "    HfArgumentParser,\n",
    "    PreTrainedTokenizerFast,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    "    AdamW,\n",
    "    get_scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments hidden in next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name_or_path: str = field(\n",
    "        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n",
    "    )\n",
    "    config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
    "    )\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
    "    )\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n",
    "    )\n",
    "    model_revision: str = field(\n",
    "        default=\"main\",\n",
    "        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n",
    "    )\n",
    "    use_auth_token: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": \"Will use the token generated when running `transformers-cli login` (necessary to use this script \"\n",
    "            \"with private models).\"\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    \"\"\"\n",
    "\n",
    "    task_name: Optional[str] = field(default=\"ner\", metadata={\"help\": \"The name of the task (ner, pos...).\"})\n",
    "    dataset_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    dataset_config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    train_file: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The input training data file (a csv or JSON file).\"}\n",
    "    )\n",
    "    validation_file: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"An optional input evaluation data file to evaluate on (a csv or JSON file).\"},\n",
    "    )\n",
    "    test_file: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"An optional input test data file to predict on (a csv or JSON file).\"},\n",
    "    )\n",
    "    text_column_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The column name of text to input in the file (a csv or JSON file).\"}\n",
    "    )\n",
    "    label_column_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The column name of label to input in the file (a csv or JSON file).\"}\n",
    "    )\n",
    "    overwrite_cache: bool = field(\n",
    "        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n",
    "    )\n",
    "    preprocessing_num_workers: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n",
    "    )\n",
    "    max_seq_length: int = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"The maximum total input sequence length after tokenization. If set, sequences longer \"\n",
    "            \"than this will be truncated, sequences shorter will be padded.\"\n",
    "        },\n",
    "    )\n",
    "    pad_to_max_length: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": \"Whether to pad all samples to model maximum sentence length. \"\n",
    "            \"If False, will pad the samples dynamically when batching to the maximum length in the batch. More \"\n",
    "            \"efficient on GPU but very bad for TPU.\"\n",
    "        },\n",
    "    )\n",
    "    max_train_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n",
    "            \"value if set.\"\n",
    "        },\n",
    "    )\n",
    "    max_eval_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n",
    "            \"value if set.\"\n",
    "        },\n",
    "    )\n",
    "    max_predict_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"For debugging purposes or quicker training, truncate the number of prediction examples to this \"\n",
    "            \"value if set.\"\n",
    "        },\n",
    "    )\n",
    "    label_all_tokens: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": \"Whether to put the label for one word on all tokens of generated by that word or just on the \"\n",
    "            \"one (in which case the other tokens will have a padding index).\"\n",
    "        },\n",
    "    )\n",
    "    return_entity_level_metrics: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Whether to return all the entity levels during evaluation or just the overall ones.\"},\n",
    "    )\n",
    "\n",
    "    use_bio: bool = field(default=True, metadata={\"help\": \"If True, use BIO tagging. Otherwise use IO.\"})\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.dataset_name is None and self.train_file is None and self.validation_file is None:\n",
    "            raise ValueError(\"Need either a dataset name or a training/validation file.\")\n",
    "        else:\n",
    "            if self.train_file is not None:\n",
    "                extension = self.train_file.split(\".\")[-1]\n",
    "                assert extension in [\"csv\", \"json\"], \"`train_file` should be a csv or a json file.\"\n",
    "            if self.validation_file is not None:\n",
    "                extension = self.validation_file.split(\".\")[-1]\n",
    "                assert extension in [\"csv\", \"json\"], \"`validation_file` should be a csv or a json file.\"\n",
    "        self.task_name = self.task_name.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = ModelArguments(\n",
    "    model_name_or_path=\"google/bigbird-roberta-base\",\n",
    ")\n",
    "data_args = DataTrainingArguments(\n",
    "    train_file=\"train.json\",\n",
    "    validation_file=\"validation.json\",\n",
    "    text_column_name=\"words\",\n",
    "    label_column_name=\"bio\",\n",
    "    max_seq_length=1024,\n",
    "    pad_to_max_length=True,\n",
    "    label_all_tokens=True,\n",
    "    return_entity_level_metrics=True,\n",
    "    use_bio=True,\n",
    ")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"feedback-prize-bigbird-base\",\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    do_predict=True,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.0095,\n",
    "    num_train_epochs=5,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    warmup_steps=200,\n",
    "    logging_steps=50,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_CV F1\",\n",
    "    seed=1234,\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    "    run_name=\"bigbird-base-ner\",\n",
    "    adam_beta2=0.999,\n",
    "    adam_epsilon=1e-6,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 80-20 data split\n",
    "\n",
    "Data is in BIO format.  \n",
    "Here is a notebook showing how to make the data:  https://www.kaggle.com/nbroad/feedback-prize-bio-format-for-ner  \n",
    "Here is a dataset to use in your own notebooks: https://www.kaggle.com/nbroad/feedbackprize-bio-ner-train-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = datasets.load_dataset(\"json\", data_files=\"./data/bio.json\", split=\"train\")\n",
    "\n",
    "split_dataset = full_dataset.train_test_split(test_size=0.2, seed=training_args.seed)\n",
    "\n",
    "split_dataset[\"train\"].to_json(\"train.json\")\n",
    "split_dataset[\"test\"].to_json(\"validation.json\")\n",
    "\n",
    "ground_truth_dataset = load_dataset(\"csv\", data_files=\"./data/train.csv\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    handlers=[logging.StreamHandler()],\n",
    ")\n",
    "\n",
    "log_level = training_args.get_process_log_level()\n",
    "datasets.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.enable_default_handler()\n",
    "transformers.utils.logging.enable_explicit_format()\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(log_level)\n",
    "\n",
    "# Log on each process the small summary:\n",
    "logger.warning(\n",
    "    f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n",
    "    + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n",
    ")\n",
    "logger.info(f\"Training/evaluation parameters {training_args}\")\n",
    "\n",
    "set_seed(training_args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {}\n",
    "if data_args.train_file is not None:\n",
    "    data_files[\"train\"] = data_args.train_file\n",
    "if data_args.validation_file is not None:\n",
    "    data_files[\"validation\"] = data_args.validation_file\n",
    "if data_args.test_file is not None:\n",
    "    data_files[\"test\"] = data_args.test_file\n",
    "extension = data_args.train_file.split(\".\")[-1]\n",
    "raw_datasets = load_dataset(extension, data_files=data_files, cache_dir=model_args.cache_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if training_args.do_train:\n",
    "    column_names = raw_datasets[\"train\"].column_names\n",
    "    features = raw_datasets[\"train\"].features\n",
    "else:\n",
    "    column_names = raw_datasets[\"validation\"].column_names\n",
    "    features = raw_datasets[\"validation\"].features\n",
    "\n",
    "if data_args.text_column_name is not None:\n",
    "    text_column_name = data_args.text_column_name\n",
    "elif \"tokens\" in column_names:\n",
    "    text_column_name = \"tokens\"\n",
    "else:\n",
    "    text_column_name = column_names[0]\n",
    "\n",
    "if data_args.label_column_name is not None:\n",
    "    label_column_name = data_args.label_column_name\n",
    "elif f\"{data_args.task_name}_tags\" in column_names:\n",
    "    label_column_name = f\"{data_args.task_name}_tags\"\n",
    "else:\n",
    "    label_column_name = column_names[1]\n",
    "\n",
    "# In the event the labels are not a `Sequence[ClassLabel]`, we will need to go through the dataset to get the\n",
    "# unique labels.\n",
    "def get_label_list(labels):\n",
    "    unique_labels = set()\n",
    "    for label in labels:\n",
    "        unique_labels = unique_labels | set(label)\n",
    "    label_list = list(unique_labels)\n",
    "    label_list.sort()\n",
    "    return label_list\n",
    "\n",
    "if isinstance(features[label_column_name].feature, ClassLabel):\n",
    "    label_list = features[label_column_name].feature.names\n",
    "    # No need to convert the labels since they are already ints.\n",
    "    label_to_id = {i: i for i in range(len(label_list))}\n",
    "else:\n",
    "    label_list = get_label_list(raw_datasets[\"train\"][label_column_name])\n",
    "    label_to_id = {l: i for i, l in enumerate(label_list)}\n",
    "num_labels = len(label_list)\n",
    "\n",
    "if not data_args.use_bio:\n",
    "    # Map that sends B-Xxx label to its I-Xxx counterpart\n",
    "    b_to_i_label = []\n",
    "    for idx, label in enumerate(label_list):\n",
    "        if label.startswith(\"B-\") and label.replace(\"B-\", \"I-\") in label_list:\n",
    "            b_to_i_label.append(label_list.index(label.replace(\"B-\", \"I-\")))\n",
    "        else:\n",
    "            b_to_i_label.append(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config, Tokenizer, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(\n",
    "    model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n",
    "    num_labels=num_labels,\n",
    "    label2id=label_to_id,\n",
    "    id2label={i: l for l, i in label_to_id.items()},\n",
    "    finetuning_task=data_args.task_name,\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    revision=model_args.model_revision,\n",
    "    use_auth_token=True if model_args.use_auth_token else None,\n",
    ")\n",
    "\n",
    "tokenizer_name_or_path = model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    tokenizer_name_or_path,\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    use_fast=True,\n",
    "    add_prefix_space=True,\n",
    "    revision=model_args.model_revision,\n",
    "    use_auth_token=True if model_args.use_auth_token else None,\n",
    ")\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
    "    config=config,\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    revision=model_args.model_revision,\n",
    "    use_auth_token=True if model_args.use_auth_token else None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the dataset\n",
    "# Padding strategy\n",
    "padding = \"max_length\" if data_args.pad_to_max_length else False\n",
    "\n",
    "# Tokenize all texts and align the labels with them.\n",
    "def tokenize_and_align_labels(examples, include_word_ids=False):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[data_args.text_column_name],\n",
    "        padding=padding,\n",
    "        truncation=True,\n",
    "        max_length=data_args.max_seq_length,\n",
    "        # We use this argument because the texts in our dataset are lists of words (with a label for each word).\n",
    "        is_split_into_words=True,\n",
    "    )\n",
    "    labels = []\n",
    "    all_word_ids = []\n",
    "    for i, label in enumerate(examples[data_args.label_column_name]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label_to_id[label[word_idx]])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                if data_args.label_all_tokens:\n",
    "                    # The default script converts all b-labels to i-labels\n",
    "                    if data_args.use_bio:\n",
    "                        label_ids.append(label_to_id[label[word_idx]])\n",
    "                    else:\n",
    "                        label_ids.append(b_to_i_label[label_to_id[label[word_idx]]])\n",
    "                else:\n",
    "                    label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "        if include_word_ids:\n",
    "            all_word_ids.append(word_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    \n",
    "    if include_word_ids:\n",
    "        tokenized_inputs[\"word_ids\"] = all_word_ids\n",
    "    \n",
    "    return tokenized_inputs\n",
    "\n",
    "if training_args.do_train:\n",
    "    if \"train\" not in raw_datasets:\n",
    "        raise ValueError(\"--do_train requires a train dataset\")\n",
    "    train_dataset = raw_datasets[\"train\"]\n",
    "    if data_args.max_train_samples is not None:\n",
    "        train_dataset = train_dataset.select(range(data_args.max_train_samples))\n",
    "    with training_args.main_process_first(desc=\"train dataset map pre-processing\"):\n",
    "        train_dataset = train_dataset.map(\n",
    "            partial(tokenize_and_align_labels, include_word_ids=False),\n",
    "            batched=True,\n",
    "            num_proc=data_args.preprocessing_num_workers,\n",
    "            load_from_cache_file=not data_args.overwrite_cache,\n",
    "            desc=\"Running tokenizer on train dataset\",\n",
    "        )\n",
    "\n",
    "if training_args.do_eval:\n",
    "    if \"validation\" not in raw_datasets:\n",
    "        raise ValueError(\"--do_eval requires a validation dataset\")\n",
    "    eval_dataset = raw_datasets[\"validation\"]\n",
    "    if data_args.max_eval_samples is not None:\n",
    "        eval_dataset = eval_dataset.select(range(data_args.max_eval_samples))\n",
    "    with training_args.main_process_first(desc=\"validation dataset map pre-processing\"):\n",
    "        eval_dataset = eval_dataset.map(\n",
    "            partial(tokenize_and_align_labels, include_word_ids=True),\n",
    "            batched=True,\n",
    "            num_proc=data_args.preprocessing_num_workers,\n",
    "            load_from_cache_file=not data_args.overwrite_cache,\n",
    "            desc=\"Running tokenizer on validation dataset\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collator and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seqeval\n",
    "# Data collator\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=8 if training_args.fp16 else None)\n",
    "\n",
    "# Metrics\n",
    "metric = load_metric(\"metric.py\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    if data_args.return_entity_level_metrics:\n",
    "        # Unpack nested dictionaries\n",
    "        final_results = {}\n",
    "        for key, value in results.items():\n",
    "            if isinstance(value, dict):\n",
    "                for n, v in value.items():\n",
    "                    final_results[f\"{key}_{n}\"] = v\n",
    "            else:\n",
    "                final_results[key] = value\n",
    "        return final_results\n",
    "    else:\n",
    "        return {\n",
    "            \"precision\": results[\"overall_precision\"],\n",
    "            \"recall\": results[\"overall_recall\"],\n",
    "            \"f1\": results[\"overall_f1\"],\n",
    "            \"accuracy\": results[\"overall_accuracy\"],\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV Calculation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rob Mulla @robikscube\n",
    "# https://www.kaggle.com/robikscube/student-writing-competition-twitch\n",
    "def calc_overlap(pred, ground_truth):\n",
    "    \"\"\"\n",
    "    Calculates the overlap between prediction and\n",
    "    ground truth and overlap percentages used for determining\n",
    "    true positives.\n",
    "    \"\"\"\n",
    "    set_pred = set(pred.split(' '))\n",
    "    set_gt = set(ground_truth.split(' '))\n",
    "    # Length of each and intersection\n",
    "    len_gt = len(set_gt)\n",
    "    len_pred = len(set_pred)\n",
    "    inter = len(set_gt.intersection(set_pred))\n",
    "    overlap_1 = inter / len_gt\n",
    "    overlap_2 = inter/ len_pred\n",
    "    return [overlap_1, overlap_2]\n",
    "\n",
    "\n",
    "def score_feedback_comp(pred_df, gt_df):\n",
    "    \"\"\"\n",
    "    A function that scores for the kaggle\n",
    "        Student Writing Competition\n",
    "        \n",
    "    Uses the steps in the evaluation page here:\n",
    "        https://www.kaggle.com/c/feedback-prize-2021/overview/evaluation\n",
    "    \"\"\"\n",
    "    gt_df = gt_df[['id','discourse_type','predictionstring']] \\\n",
    "        .reset_index(drop=True).copy()\n",
    "    pred_df = pred_df[['id','class','predictionstring']] \\\n",
    "        .reset_index(drop=True).copy()\n",
    "    pred_df['pred_id'] = pred_df.index\n",
    "    gt_df['gt_id'] = gt_df.index\n",
    "    # Step 1. all ground truths and predictions for a given class are compared.\n",
    "    joined = pred_df.merge(gt_df,\n",
    "                           left_on=['id','class'],\n",
    "                           right_on=['id','discourse_type'],\n",
    "                           how='outer',\n",
    "                           suffixes=('_pred','_gt')\n",
    "                          )\n",
    "    joined['predictionstring_gt'] = joined['predictionstring_gt'].fillna(' ')\n",
    "    joined['predictionstring_pred'] = joined['predictionstring_pred'].fillna(' ')\n",
    "\n",
    "    joined['overlaps'] = [calc_overlap(pred, gt) for pred, gt in joined[['predictionstring_pred', 'predictionstring_gt']].values]\n",
    "\n",
    "    # 2. If the overlap between the ground truth and prediction is >= 0.5, \n",
    "    # and the overlap between the prediction and the ground truth >= 0.5,\n",
    "    # the prediction is a match and considered a true positive.\n",
    "    # If multiple matches exist, the match with the highest pair of overlaps is taken.\n",
    "    joined['overlap1'] = joined['overlaps'].apply(lambda x: eval(str(x))[0])\n",
    "    joined['overlap2'] = joined['overlaps'].apply(lambda x: eval(str(x))[1])\n",
    "\n",
    "\n",
    "    joined['potential_TP'] = (joined['overlap1'] >= 0.5) & (joined['overlap2'] >= 0.5)\n",
    "    joined['max_overlap'] = joined[['overlap1','overlap2']].max(axis=1)\n",
    "    tp_pred_ids = joined.query('potential_TP') \\\n",
    "        .sort_values('max_overlap', ascending=False) \\\n",
    "        .groupby(['id','predictionstring_gt']).first()['pred_id'].values\n",
    "\n",
    "    # 3. Any unmatched ground truths are false negatives\n",
    "    # and any unmatched predictions are false positives.\n",
    "    fp_pred_ids = [p for p in joined['pred_id'].unique() if p not in tp_pred_ids]\n",
    "\n",
    "    matched_gt_ids = joined.query('potential_TP')['gt_id'].unique()\n",
    "    unmatched_gt_ids = [c for c in joined['gt_id'].unique() if c not in matched_gt_ids]\n",
    "\n",
    "    # Get numbers of each type\n",
    "    TP = len(tp_pred_ids)\n",
    "    FP = len(fp_pred_ids)\n",
    "    FN = len(unmatched_gt_ids)\n",
    "    #calc microf1\n",
    "    my_f1_score = TP / (TP + 0.5*(FP+FN))\n",
    "    return round(my_f1_score, 4)\n",
    "\n",
    "\n",
    "id2label={i: l for l, i in label_to_id.items()}\n",
    "# https://www.kaggle.com/zzy990106/pytorch-ner-infer?scriptVersionId=82677278&cellId=13\n",
    "def get_label_predictions(dataset, preds):\n",
    "\n",
    "    ids = dataset[\"id\"]\n",
    "    word_ids = dataset[\"word_ids\"]\n",
    "    words = dataset[\"words\"]\n",
    "    \n",
    "    all_preds = []\n",
    "\n",
    "    for id_, sample_preds, sample_word_ids, words in zip(ids, preds, word_ids, words):\n",
    "        label_preds = [\"\"]*len(words)\n",
    "\n",
    "        for pred, w_id in zip(sample_preds, sample_word_ids):\n",
    "            if w_id is None:\n",
    "                continue\n",
    "            if label_preds[w_id] == \"\":\n",
    "                label_preds[w_id] = id2label[pred]\n",
    "\n",
    "\n",
    "        temp_preds = []\n",
    "        j = 0\n",
    "        while j < len(label_preds):\n",
    "            label = label_preds[j]\n",
    "            if label == 'O' or label == '' or label[0]== 'I':\n",
    "                j += 1\n",
    "            else:\n",
    "                end = j + 1\n",
    "                while end < len(label_preds) and label_preds[end].replace('B-','').replace('I-','') == label.replace('B-','').replace('I-',''):\n",
    "                    end += 1\n",
    "\n",
    "                if end - j > 5:\n",
    "                    all_preds.append((id_, label.replace('B-',''), ' '.join(map(str, list(range(j, end))))))\n",
    "\n",
    "                j = end\n",
    "    return all_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer, LR Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\"params\": [p for n, p in model.named_parameters()\n",
    "                if not any(nd in n for nd in no_decay)],\n",
    "     \"weight_decay\": config['weight_decay'],\n",
    "     },\n",
    "    {\"params\": [p for n, p in model.named_parameters()\n",
    "                if any(nd in n for nd in no_decay)],\n",
    "     \"weight_decay\": 0.0,\n",
    "     },\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                  lr=config['lr'],\n",
    "                  betas=(0.9, training_args.adam_beta2),\n",
    "                  eps=training_args.adam_epsilon\n",
    "                  )\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=training_args.lr_scheduler_type,\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=training_args.warmup_steps,\n",
    "    num_training_steps=training_args.num_train_epochs * len(train_dataset) /  training_args.gradient_accumulation_steps, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedbackPrizeTrainer(Trainer):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        # The Trianer will remove the important columns needed for cv from the eval_dataset,\n",
    "        # so we'll just store it like this\n",
    "        if \"cv_dataset\" in kwargs:\n",
    "            self.cv_dataset = kwargs.pop(\"cv_dataset\")\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "\n",
    "    def evaluation_loop(\n",
    "        self,\n",
    "        dataloader,\n",
    "        description,\n",
    "        prediction_loss_only = None,\n",
    "        ignore_keys = None,\n",
    "        metric_key_prefix = \"eval\",\n",
    "    ):\n",
    "        \n",
    "        eval_output =  super().evaluation_loop(\n",
    "            dataloader,\n",
    "            description,\n",
    "            prediction_loss_only,\n",
    "            ignore_keys,\n",
    "            metric_key_prefix\n",
    "        )\n",
    "        \n",
    "        is_in_eval = metric_key_prefix == \"eval\"\n",
    "        \n",
    "        # Custom CV F1 calculation\n",
    "        if is_in_eval:\n",
    "\n",
    "            eval_id_preds = eval_output.predictions.argmax(-1)\n",
    "            \n",
    "            eval_label_preds = get_label_predictions(self.cv_dataset, eval_id_preds)\n",
    "            \n",
    "            eval_pred_df = pd.DataFrame(eval_label_preds, columns=[\"id\", \"class\", \"predictionstring\"])\n",
    "            ground_truth_df = ground_truth_dataset.to_pandas()\n",
    "            \n",
    "            eval_gt_df = ground_truth_df[ground_truth_df[\"id\"].isin(self.cv_dataset[\"id\"])]\n",
    "\n",
    "            eval_f1_score = score_feedback_comp(eval_pred_df, eval_gt_df)\n",
    "            \n",
    "            eval_output.metrics[f\"{metric_key_prefix}_CV F1\"] = eval_f1_score\n",
    "#             logger.info(f\"CV F1 = {eval_f1_score}\")  # This doesn't work\n",
    "#             print(f\"CV F1 at global step {self.state.global_step} = {eval_f1_score}\") # I couldn't figure out how to print it with `logging`. Do share if you know why\n",
    "        \n",
    "        return eval_output\n",
    "\n",
    "# Initialize our Trainer\n",
    "trainer = FeedbackPrizeTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset if training_args.do_train else None,\n",
    "    eval_dataset=eval_dataset if training_args.do_eval else None,\n",
    "    cv_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    optimizers=(optimizer, lr_scheduler),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "### Scores for each evaluation will be shown in a table. By setting `return_entity_level_metrics=True`, you can see the metrics for each discourse type. If you go all the way to the right on the table, you'll see the CV F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env TOKENIZERS_PARALLELISM=true\n",
    "\n",
    "# Training\n",
    "if training_args.do_train:\n",
    "    train_result = trainer.train()\n",
    "    metrics = train_result.metrics\n",
    "    trainer.save_model()  # Saves the tokenizer too for easy upload\n",
    "\n",
    "    max_train_samples = (\n",
    "        data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)\n",
    "    )\n",
    "    metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
    "\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    trainer.save_metrics(\"train\", metrics)\n",
    "    trainer.save_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "if training_args.do_eval:\n",
    "    logger.info(\"*** Evaluate ***\")\n",
    "\n",
    "    metrics = trainer.evaluate()\n",
    "\n",
    "    max_eval_samples = data_args.max_eval_samples if data_args.max_eval_samples is not None else len(eval_dataset)\n",
    "    metrics[\"eval_samples\"] = min(max_eval_samples, len(eval_dataset))\n",
    "\n",
    "    trainer.log_metrics(\"eval\", metrics)\n",
    "    trainer.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = {\n",
    "    \"id\": [],\n",
    "    data_args.text_column_name: []\n",
    "}\n",
    "\n",
    "def tokenize(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[data_args.text_column_name],\n",
    "        padding=padding,\n",
    "        truncation=True,\n",
    "        max_length=data_args.max_seq_length,\n",
    "        # We use this argument because the texts in our dataset are lists of words (with a label for each word).\n",
    "        is_split_into_words=True,\n",
    "    )\n",
    "    num_samples = len(tokenized_inputs[\"input_ids\"])\n",
    "    tokenized_inputs[\"word_ids\"] = [tokenized_inputs.word_ids(batch_index=i) for i in range(num_samples)]\n",
    "    \n",
    "    return tokenized_inputs\n",
    "\n",
    "# If you actually want to submit, it would probably be best in another notebook.\n",
    "# Take your model, stick it in the Trainer, and run it through this to get your submission file.\n",
    "# A lot of the code above is for training and validation, so only copy what you need.\n",
    "if training_args.do_predict:\n",
    "    for file in Path(\"../input/feedback-prize-2021/test\").glob(\"*.txt\"):\n",
    "        file_id = file.stem\n",
    "\n",
    "        with open(file) as fp:\n",
    "            text = fp.read()\n",
    "\n",
    "        test_data[data_args.text_column_name].append(text.split())\n",
    "        test_data[\"id\"].append(file_id)\n",
    "\n",
    "    raw_test_dataset = Dataset.from_dict(test_data)\n",
    "\n",
    "    test_dataset = raw_test_dataset.map(\n",
    "            tokenize,\n",
    "            batched=True,\n",
    "            num_proc=data_args.preprocessing_num_workers,\n",
    "            load_from_cache_file=not data_args.overwrite_cache,\n",
    "            desc=\"Running tokenizer on test dataset\",\n",
    "        )\n",
    "\n",
    "    test_predictions = trainer.predict(test_dataset)\n",
    "    \n",
    "    test_id_preds = test_predictions.predictions.argmax(-1)\n",
    "    \n",
    "    test_label_preds = get_label_predictions(test_dataset, test_id_preds)\n",
    "    test_pred_df = pd.DataFrame(test_label_preds, columns=[\"id\", \"class\", \"predictionstring\"])\n",
    "    \n",
    "    test_pred_df.to_csv(\"submission.csv\", index=False)\n",
    "    \n",
    "    display(test_pred_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
