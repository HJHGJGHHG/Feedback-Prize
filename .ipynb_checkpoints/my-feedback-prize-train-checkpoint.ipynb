{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, AdamW, get_scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'fold_num': 5,\n",
    "    'seed': 1234,\n",
    "    #'model': 'roberta-base',\n",
    "    #'model': '../input/robertalarge',\n",
    "    #'model': 'allenai/longformer-base-4096',\n",
    "    #'model': 'allenai/longformer-large-4096',\n",
    "    'model': 'google/bigbird-roberta-base',\n",
    "    'max_len': 1024,\n",
    "    'epochs': 5,\n",
    "    'train_bs': 3,\n",
    "    'valid_bs': 6,\n",
    "    'lr': 1e-5/4,\n",
    "    'num_workers': 0,\n",
    "    'weight_decay': 1e-6,\n",
    "    'num_warmup_steps': 500,\n",
    "    'lr_scheduler_type': 'linear',\n",
    "    'gradient_accumulation_steps': 8,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['O', 'B-Lead', 'I-Lead', 'B-Position', 'I-Position', 'B-Claim', 'I-Claim', 'B-Counterclaim', 'I-Counterclaim',\n",
    "          'B-Rebuttal', 'I-Rebuttal', 'B-Evidence', 'I-Evidence', 'B-Concluding Statement', 'I-Concluding Statement']\n",
    "labels2index = {\n",
    "    'Lead': 1, 'Position': 3, 'Claim': 5, 'Counterclaim': 7, 'Rebuttal': 9, 'Evidence': 11, 'Concluding Statement': 13\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "set_seed(config['seed'])\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>discourse_start</th>\n",
       "      <th>discourse_end</th>\n",
       "      <th>discourse_text</th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>discourse_type_num</th>\n",
       "      <th>predictionstring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>8.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>Modern humans today are always on their phone....</td>\n",
       "      <td>Lead</td>\n",
       "      <td>Lead 1</td>\n",
       "      <td>1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>230.0</td>\n",
       "      <td>312.0</td>\n",
       "      <td>They are some really bad consequences when stu...</td>\n",
       "      <td>Position</td>\n",
       "      <td>Position 1</td>\n",
       "      <td>45 46 47 48 49 50 51 52 53 54 55 56 57 58 59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  discourse_id  discourse_start  discourse_end  \\\n",
       "0  423A1CA112E2  1.622628e+12              8.0          229.0   \n",
       "1  423A1CA112E2  1.622628e+12            230.0          312.0   \n",
       "\n",
       "                                      discourse_text discourse_type  \\\n",
       "0  Modern humans today are always on their phone....           Lead   \n",
       "1  They are some really bad consequences when stu...       Position   \n",
       "\n",
       "  discourse_type_num                                   predictionstring  \n",
       "0             Lead 1  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...  \n",
       "1         Position 1       45 46 47 48 49 50 51 52 53 54 55 56 57 58 59  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('./data/train.csv')\n",
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15594/15594 [00:00<00:00, 17985.61it/s]\n"
     ]
    }
   ],
   "source": [
    "train_names, train_texts = [], []\n",
    "for f in tqdm(list(os.listdir('./data/train'))):\n",
    "    train_names.append(f.replace('.txt', ''))\n",
    "    with open('./data/train/' + f, 'r', encoding='utf-8') as f:\n",
    "        text = ''\n",
    "        for line in f.readlines():\n",
    "            #text += line.replace('\\n', '').replace('\\xa0', '')\n",
    "            text += line.replace('\\n', ' ')\n",
    "        train_texts.append(text)\n",
    "train_texts = pd.DataFrame({'id': train_names, 'text': train_texts})\n",
    "train_texts['text'] = train_texts['text'].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_texts = train_texts.sort_values(by='id').reset_index(drop=True)\n",
    "train_df = train_df.sort_values(by=['id', 'discourse_start']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 20000 discourses.\n",
      "Processed 40000 discourses.\n",
      "Processed 60000 discourses.\n",
      "Processed 80000 discourses.\n",
      "Processed 100000 discourses.\n",
      "Processed 120000 discourses.\n",
      "Processed 140000 discourses.\n"
     ]
    }
   ],
   "source": [
    "text_index = dict.fromkeys(train_texts['id'].values.tolist())\n",
    "for i in range(len(train_df)):\n",
    "    id = train_df.iloc[i]['id']\n",
    "    if not text_index[id]:\n",
    "        text_index[id] = [i]\n",
    "    else:\n",
    "        text_index[id].append(i)\n",
    "    if (i+1) % 20000 == 0:\n",
    "        print(\"Processed {0} discourses.\".format(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2000 essays.\n",
      "Processed 4000 essays.\n",
      "Processed 6000 essays.\n",
      "Processed 8000 essays.\n",
      "Processed 10000 essays.\n",
      "Processed 12000 essays.\n",
      "Processed 14000 essays.\n"
     ]
    }
   ],
   "source": [
    "taggings = []\n",
    "essays = 0\n",
    "for i in range(len(train_texts)):\n",
    "    text_id = train_texts.iloc[i]['id']\n",
    "    text = train_texts.iloc[i]['text']\n",
    "    tagging = [0] * config['max_len']\n",
    "    for k in text_index[text_id]:\n",
    "        if train_df.iloc[k]['id'] != train_texts.iloc[i]['id']:\n",
    "            break\n",
    "\n",
    "        discourse_type = train_df.iloc[k]['discourse_type']\n",
    "        predictionstring = train_df.iloc[k]['predictionstring'].split(' ')\n",
    "        label = labels2index[discourse_type]\n",
    "        if int(predictionstring[0]) > config['max_len'] - 2:\n",
    "            break\n",
    "        else:\n",
    "            tagging[int(predictionstring[0]) + 1] = label\n",
    "        for m in range(int(predictionstring[0]) + 2, int(predictionstring[-1]) + 2):\n",
    "            if m > config['max_len'] - 2:\n",
    "                break\n",
    "            else:\n",
    "                tagging[m] = label + 1\n",
    "    tagging[-1] = 0\n",
    "    taggings.append(tagging)\n",
    "    essays += 1\n",
    "    if essays % 2000 == 0:\n",
    "        print(\"Processed {0} essays.\".format(essays))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts['tagging'] = taggings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config['model'], add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, df, phase='Train'):\n",
    "        self.df = df\n",
    "        self.phase = phase\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df.text.values[idx]\n",
    "        if self.phase == 'Train':\n",
    "            label = self.df.tagging.values[idx]\n",
    "            return {'text': text, 'label': label}\n",
    "        else:\n",
    "            return {'text': text}\n",
    "\n",
    "\n",
    "def collate_fn(data):\n",
    "    input_ids, attention_mask = [], []\n",
    "    text = [item['text'] for item in data]\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "        max_length=config['max_len'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    words = []\n",
    "    for i in range(len(data)):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        words.append(word_ids)\n",
    "\n",
    "    tokenized_inputs[\"word_ids\"] = words\n",
    "    if 'label' in data[0].keys():\n",
    "        label = [item['label'] for item in data]\n",
    "        tokenized_inputs['labels'] = torch.LongTensor(label)\n",
    "\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(train_texts, phase='Train')\n",
    "train_iter = DataLoader(train_dataset, batch_size=config['train_bs'], collate_fn=collate_fn, shuffle=False,\n",
    "                        num_workers=config['num_workers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Prepare Optimizer and LR Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForTokenClassification: ['cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BigBirdForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BigBirdForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BigBirdForTokenClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(config['model'], num_labels=15).to(device)\n",
    "\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\"params\": [p for n, p in model.named_parameters()\n",
    "                if not any(nd in n for nd in no_decay)],\n",
    "     \"weight_decay\": config['weight_decay'],\n",
    "     },\n",
    "    {\"params\": [p for n, p in model.named_parameters()\n",
    "                if any(nd in n for nd in no_decay)],\n",
    "     \"weight_decay\": 0.0,\n",
    "     },\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                  #lr=config['lr'],\n",
    "                  lr=config['lr'] * config['gradient_accumulation_steps'],\n",
    "                  betas=(0.9, 0.999),\n",
    "                  eps=1e-6\n",
    "                  )\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=config['lr_scheduler_type'],\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=config['num_warmup_steps'],\n",
    "    num_training_steps=config['epochs'] * len(train_iter) /  config['gradient_accumulation_steps'], )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5198 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 99/5198 [01:22<1:09:51,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 Step 100/5198  Average Training Loss:2.781197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 199/5198 [02:44<1:08:16,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 Step 200/5198  Average Training Loss:2.769809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 299/5198 [04:06<1:06:28,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 Step 300/5198  Average Training Loss:2.747930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 399/5198 [05:28<1:05:02,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 Step 400/5198  Average Training Loss:2.716120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 499/5198 [06:50<1:03:28,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 Step 500/5198  Average Training Loss:2.672472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 599/5198 [08:12<1:02:14,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 Step 600/5198  Average Training Loss:2.604754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 699/5198 [09:35<1:00:14,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 Step 700/5198  Average Training Loss:2.502904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 799/5198 [10:57<59:43,  1.23it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 Step 800/5198  Average Training Loss:2.410652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 899/5198 [12:19<58:51,  1.22it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 Step 900/5198  Average Training Loss:2.332454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 999/5198 [13:41<58:06,  1.20it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 Step 1000/5198  Average Training Loss:2.261404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 1099/5198 [15:03<55:43,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 Step 1100/5198  Average Training Loss:2.194715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 1199/5198 [16:24<54:55,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 Step 1200/5198  Average Training Loss:2.129232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 1299/5198 [17:46<53:36,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 Step 1300/5198  Average Training Loss:2.067647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 1399/5198 [19:09<52:01,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 Step 1400/5198  Average Training Loss:2.011991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 1499/5198 [20:31<50:52,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 Step 1500/5198  Average Training Loss:1.957682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 1599/5198 [21:53<48:58,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 Step 1600/5198  Average Training Loss:1.911867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1699/5198 [23:15<48:37,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 Step 1700/5198  Average Training Loss:1.867177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 1799/5198 [24:37<46:36,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 Step 1800/5198  Average Training Loss:1.826661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 1899/5198 [25:59<43:53,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 Step 1900/5198  Average Training Loss:1.786865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 1999/5198 [27:18<42:28,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 Step 2000/5198  Average Training Loss:1.750513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2099/5198 [28:38<40:35,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 Step 2100/5198  Average Training Loss:1.718320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 2199/5198 [29:59<39:55,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 Step 2200/5198  Average Training Loss:1.688921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 2299/5198 [31:18<37:50,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 Step 2300/5198  Average Training Loss:1.660923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 2399/5198 [32:38<37:56,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 Step 2400/5198  Average Training Loss:1.638128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 2499/5198 [33:57<35:08,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 Step 2500/5198  Average Training Loss:1.614818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2599/5198 [35:16<33:26,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 Step 2600/5198  Average Training Loss:1.593653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 2699/5198 [36:36<33:02,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 Step 2700/5198  Average Training Loss:1.572342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 2799/5198 [37:56<31:51,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 Step 2800/5198  Average Training Loss:1.552976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 2899/5198 [39:15<30:25,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 Step 2900/5198  Average Training Loss:1.534945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 2999/5198 [40:35<28:54,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 Step 3000/5198  Average Training Loss:1.515842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 3099/5198 [41:55<27:10,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 Step 3100/5198  Average Training Loss:1.498769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 3199/5198 [43:14<26:38,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 Step 3200/5198  Average Training Loss:1.482969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 3299/5198 [44:33<24:59,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 Step 3300/5198  Average Training Loss:1.468479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 3399/5198 [45:52<23:48,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 Step 3400/5198  Average Training Loss:1.455250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 3499/5198 [47:13<23:23,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 Step 3500/5198  Average Training Loss:1.441690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 3599/5198 [48:32<21:18,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 Step 3600/5198  Average Training Loss:1.427863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 3699/5198 [49:52<20:18,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 Step 3700/5198  Average Training Loss:1.416275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 3799/5198 [51:13<18:44,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 Step 3800/5198  Average Training Loss:1.405741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3899/5198 [52:32<16:48,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 Step 3900/5198  Average Training Loss:1.395064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 3999/5198 [53:52<15:49,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 Step 4000/5198  Average Training Loss:1.385499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 4099/5198 [55:12<14:20,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 Step 4100/5198  Average Training Loss:1.374857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 4199/5198 [56:31<12:54,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 Step 4200/5198  Average Training Loss:1.364579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 4299/5198 [57:50<11:41,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 Step 4300/5198  Average Training Loss:1.354088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 4399/5198 [59:09<10:42,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 Step 4400/5198  Average Training Loss:1.345101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 4499/5198 [1:00:28<09:18,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 Step 4500/5198  Average Training Loss:1.336652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 4599/5198 [1:01:47<07:50,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 Step 4600/5198  Average Training Loss:1.328459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 4699/5198 [1:03:07<06:35,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 Step 4700/5198  Average Training Loss:1.320562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 4799/5198 [1:04:26<05:09,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 Step 4800/5198  Average Training Loss:1.311298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 4899/5198 [1:05:46<03:58,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 Step 4900/5198  Average Training Loss:1.302986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 4999/5198 [1:07:05<02:34,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 Step 5000/5198  Average Training Loss:1.295234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 5099/5198 [1:08:24<01:17,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 Step 5100/5198  Average Training Loss:1.287996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5198/5198 [1:09:43<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5  Average Training Loss:1.281206\n",
      "Epoch 2/5\n",
      "Epoch 2/5 Step 5200/5198  Average Training Loss:1.016840\n",
      "Epoch 2/5 Step 5300/5198  Average Training Loss:0.923704\n",
      "Epoch 2/5 Step 5400/5198  Average Training Loss:0.925322\n",
      "Epoch 2/5 Step 5500/5198  Average Training Loss:0.908107\n",
      "Epoch 2/5 Step 5600/5198  Average Training Loss:0.897733\n",
      "Epoch 2/5 Step 5700/5198  Average Training Loss:0.900524\n",
      "Epoch 2/5 Step 5800/5198  Average Training Loss:0.893949\n",
      "Epoch 2/5 Step 5900/5198  Average Training Loss:0.887190\n",
      "Epoch 2/5 Step 6000/5198  Average Training Loss:0.889781\n",
      "Epoch 2/5 Step 6100/5198  Average Training Loss:0.888018\n",
      "Epoch 2/5 Step 6200/5198  Average Training Loss:0.883516\n",
      "Epoch 2/5 Step 6300/5198  Average Training Loss:0.878048\n",
      "Epoch 2/5 Step 6400/5198  Average Training Loss:0.872679\n",
      "Epoch 2/5 Step 6500/5198  Average Training Loss:0.869100\n",
      "Epoch 2/5 Step 6600/5198  Average Training Loss:0.867774\n",
      "Epoch 2/5 Step 6700/5198  Average Training Loss:0.864904\n",
      "Epoch 2/5 Step 6800/5198  Average Training Loss:0.867320\n",
      "Epoch 2/5 Step 6900/5198  Average Training Loss:0.866618\n",
      "Epoch 2/5 Step 7000/5198  Average Training Loss:0.866710\n",
      "Epoch 2/5 Step 7100/5198  Average Training Loss:0.864380\n",
      "Epoch 2/5 Step 7200/5198  Average Training Loss:0.860853\n",
      "Epoch 2/5 Step 7300/5198  Average Training Loss:0.860169\n"
     ]
    }
   ],
   "source": [
    "tk = tqdm(train_iter, total=len(train_iter), position=0, leave=True)\n",
    "model.train()\n",
    "step = 0\n",
    "model.zero_grad()\n",
    "for epoch in range(config['epochs']):\n",
    "    losses = []\n",
    "    print(\"Epoch {}/{}\".format(epoch+1, config['epochs']))\n",
    "    for batch in tk:\n",
    "        batch = {k: v.to(device) for k, v in batch.items() if k != 'word_ids'}\n",
    "        loss = model(input_ids=batch['input_ids'],\n",
    "                     attention_mask=batch['attention_mask'],\n",
    "                     labels=batch['labels']).loss\n",
    "        loss /= config['gradient_accumulation_steps']\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1, norm_type=2)\n",
    "        if (step + 1) % config['gradient_accumulation_steps'] == 0 or (step + 1) == len(train_iter) * config['epochs']:\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        step += 1\n",
    "        losses.append(loss.item() * config['gradient_accumulation_steps'])\n",
    "        if (step + 1) % 100 == 0:\n",
    "            print(\"Epoch {}/{} Step {}/{}  Average Training Loss:{:6f}\".format(\n",
    "                epoch+1,\n",
    "                config['epochs'],\n",
    "                step + 1,\n",
    "                len(train_iter),\n",
    "                np.mean(losses)))\n",
    "    # print average loss\n",
    "    print(\"Epoch {}/{}  Average Training Loss:{:6f}\".format(\n",
    "        epoch+1,\n",
    "        config['epochs'],\n",
    "        np.mean(losses)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(f'./model_trained')\n",
    "model.save_pretrained(f'./model_trained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(config['model'], num_labels=15).to(device)\n",
    "model.load_state_dict(torch.load('./model_trained/pytorch_model.bin'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('./data/sample_submission.csv')\n",
    "test_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_names, test_texts = [], []\n",
    "for f in tqdm(list(os.listdir('./data/test'))):\n",
    "    test_names.append(f.replace('.txt', ''))\n",
    "    with open('./data/test/' + f, 'r', encoding='utf-8') as f:\n",
    "        text = ''\n",
    "        for line in f.readlines():\n",
    "            #text += line.replace('\\n', '').replace('\\xa0', '')\n",
    "            text += line.replace('\\n', ' ')\n",
    "        test_texts.append(text)\n",
    "test_texts = pd.DataFrame({'id': test_names, 'text': test_texts})\n",
    "test_texts['text'] = test_texts['text'].apply(lambda x: x.split())\n",
    "test_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = MyDataset(test_texts, phase='Test')\n",
    "test_iter = DataLoader(test_dataset, batch_size=config['valid_bs'], collate_fn=collate_fn, shuffle=False,\n",
    "                        num_workers=config['num_workers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "words = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    tk = tqdm(test_iter, total=len(test_iter), position=0, leave=True)\n",
    "    for step, batch in enumerate(tk):\n",
    "        word_ids = batch['word_ids']\n",
    "        words.extend(word_ids)\n",
    "        batch = {k: v.to(device) for k, v in batch.items() if k != 'word_ids'}\n",
    "\n",
    "        output = model(input_ids=batch['input_ids'],\n",
    "                       attention_mask=batch['attention_mask']).logits\n",
    "\n",
    "        y_pred.extend(output.argmax(-1).cpu().numpy())\n",
    "        \n",
    "y_pred = np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[0][:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_preds = []\n",
    "\n",
    "for i in tqdm(range(len(test_texts))):\n",
    "    idx = test_texts.id.values[i]\n",
    "    pred = ['']*len(y_pred[i]-2)\n",
    "\n",
    "    for j in range(1, len(y_pred[i])):\n",
    "        pred[j-1] = labels[y_pred[i][j]]\n",
    "\n",
    "    pred = [x.replace('B-','').replace('I-','') for x in pred]\n",
    "    \n",
    "    j = 0\n",
    "    while j < len(pred):\n",
    "        cls = pred[j]\n",
    "        if cls == 'O':\n",
    "            j += 1\n",
    "        end = j + 1\n",
    "        while end < len(pred) and pred[end] == cls:\n",
    "            end += 1\n",
    "            \n",
    "        if cls != 'O' and cls != '' and end - j > 10:\n",
    "            final_preds.append((idx, cls, ' '.join(map(str, list(range(j, end))))))\n",
    "        \n",
    "        j = end\n",
    "        \n",
    "final_preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame(final_preds)\n",
    "sub.columns = test_df.columns\n",
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
