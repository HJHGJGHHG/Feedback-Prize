{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, AdamW, get_scheduler\n",
    "from transformers.trainer import Trainer\n",
    "from transformers.models.roberta import RobertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'fold_num': 5,\n",
    "    'seed': 42,\n",
    "    'model': 'roberta-base',\n",
    "    'max_len': 512,\n",
    "    'epochs': 1,\n",
    "    'train_bs': 8,\n",
    "    'valid_bs': 8,\n",
    "    'lr': 3e-5,\n",
    "    'num_workers': 0,\n",
    "    'weight_decay': 1e-2,\n",
    "    'num_warmup_steps': 0,\n",
    "    'lr_scheduler_type': 'linear',\n",
    "    'gradient_accumulation_steps': 1,\n",
    "}\n",
    "\n",
    "labels = ['O', 'B-Lead', 'I-Lead', 'B-Position', 'I-Position', 'B-Claim', 'I-Claim', 'B-Counterclaim', 'I-Counterclaim',\n",
    "          'B-Rebuttal', 'I-Rebuttal', 'B-Evidence', 'I-Evidence', 'B-Concluding Statement', 'I-Concluding Statement']\n",
    "labels2index = {\n",
    "    'Lead': 1, 'Position': 3, 'Claim': 5, 'Counterclaim': 7, 'Rebuttal': 9, 'Evidence': 11, 'Concluding Statement': 13\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "set_seed(config['seed'])\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>discourse_start</th>\n",
       "      <th>discourse_end</th>\n",
       "      <th>discourse_text</th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>discourse_type_num</th>\n",
       "      <th>predictionstring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>8.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>Modern humans today are always on their phone....</td>\n",
       "      <td>Lead</td>\n",
       "      <td>Lead 1</td>\n",
       "      <td>1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>230.0</td>\n",
       "      <td>312.0</td>\n",
       "      <td>They are some really bad consequences when stu...</td>\n",
       "      <td>Position</td>\n",
       "      <td>Position 1</td>\n",
       "      <td>45 46 47 48 49 50 51 52 53 54 55 56 57 58 59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  discourse_id  discourse_start  discourse_end  \\\n",
       "0  423A1CA112E2  1.622628e+12              8.0          229.0   \n",
       "1  423A1CA112E2  1.622628e+12            230.0          312.0   \n",
       "\n",
       "                                      discourse_text discourse_type  \\\n",
       "0  Modern humans today are always on their phone....           Lead   \n",
       "1  They are some really bad consequences when stu...       Position   \n",
       "\n",
       "  discourse_type_num                                   predictionstring  \n",
       "0             Lead 1  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...  \n",
       "1         Position 1       45 46 47 48 49 50 51 52 53 54 55 56 57 58 59  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('./data/train.csv')\n",
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15594/15594 [00:40<00:00, 383.28it/s] \n"
     ]
    }
   ],
   "source": [
    "train_names, train_texts = [], []\n",
    "for f in tqdm(list(os.listdir('./data/train'))):\n",
    "    train_names.append(f.replace('.txt', ''))\n",
    "    with open('./data/train/' + f, 'r', encoding='utf-8') as f:\n",
    "        text = ''\n",
    "        for line in f.readlines():\n",
    "            #text += line.replace('\\n', '').replace('\\xa0', '')\n",
    "            text += line.replace('\\n', ' ')\n",
    "        train_texts.append(text)\n",
    "train_texts = pd.DataFrame({'id': train_names, 'text': train_texts})\n",
    "train_texts['text'] = train_texts['text'].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_texts = train_texts.sort_values(by='id').reset_index(drop=True)\n",
    "train_df = train_df.sort_values(by=['id', 'discourse_start']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 20000 discourses.\n",
      "Processed 40000 discourses.\n",
      "Processed 60000 discourses.\n",
      "Processed 80000 discourses.\n",
      "Processed 100000 discourses.\n",
      "Processed 120000 discourses.\n",
      "Processed 140000 discourses.\n"
     ]
    }
   ],
   "source": [
    "text_index = dict.fromkeys(train_texts['id'].values.tolist())\n",
    "for i in range(len(train_df)):\n",
    "    id = train_df.iloc[i]['id']\n",
    "    if not text_index[id]:\n",
    "        text_index[id] = [i]\n",
    "    else:\n",
    "        text_index[id].append(i)\n",
    "    if (i + 1) % 20000 == 0:\n",
    "        print(\"Processed {0} discourses.\".format(i + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2000 essays.\n",
      "Processed 4000 essays.\n",
      "Processed 6000 essays.\n",
      "Processed 8000 essays.\n",
      "Processed 10000 essays.\n",
      "Processed 12000 essays.\n",
      "Processed 14000 essays.\n"
     ]
    }
   ],
   "source": [
    "taggings = []\n",
    "essays = 0\n",
    "for i in range(len(train_texts)):\n",
    "    text_id = train_texts.iloc[i]['id']\n",
    "    text = train_texts.iloc[i]['text']\n",
    "    tagging = [0] * config['max_len']\n",
    "    for k in text_index[text_id]:\n",
    "        if train_df.iloc[k]['id'] != train_texts.iloc[i]['id']:\n",
    "            break\n",
    "\n",
    "        discourse_type = train_df.iloc[k]['discourse_type']\n",
    "        predictionstring = train_df.iloc[k]['predictionstring'].split(' ')\n",
    "        label = labels2index[discourse_type]\n",
    "        if int(predictionstring[0]) > config['max_len'] - 2:\n",
    "            break\n",
    "        else:\n",
    "            tagging[int(predictionstring[0]) + 1] = label\n",
    "        for m in range(int(predictionstring[0]) + 2, int(predictionstring[-1]) + 2):\n",
    "            if m > config['max_len'] - 2:\n",
    "                break\n",
    "            else:\n",
    "                tagging[m] = label + 1\n",
    "    tagging[-1] = 0\n",
    "    taggings.append(tagging)\n",
    "    essays += 1\n",
    "    if essays % 2000 == 0:\n",
    "        print(\"Processed {0} essays.\".format(essays))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts['tagging'] = taggings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config['model'], add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, df, phase='Train'):\n",
    "        self.df = df\n",
    "        self.phase = phase\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df.text.values[idx]\n",
    "        if self.phase == 'Train':\n",
    "            label = self.df.tagging.values[idx]\n",
    "            return {'text': text, 'label': label}\n",
    "        else:\n",
    "            return {'text': text}\n",
    "\n",
    "\n",
    "def collate_fn(data):\n",
    "    input_ids, attention_mask = [], []\n",
    "    text = [item['text'] for item in data]\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "        max_length=config['max_len'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    words = []\n",
    "    for i in range(len(data)):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        words.append(word_ids)\n",
    "\n",
    "    tokenized_inputs[\"word_ids\"] = words\n",
    "    if 'label' in data[0].keys():\n",
    "        label = [item['label'] for item in data]\n",
    "        tokenized_inputs['labels'] = torch.LongTensor(label)\n",
    "\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(train_texts, phase='Train')\n",
    "train_iter = DataLoader(train_dataset, batch_size=config['train_bs'], collate_fn=collate_fn, shuffle=False,\n",
    "                        num_workers=config['num_workers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    0,   993,    82, 12138,  2088,    14,     5,    98,   373,    22,\n",
      "         9021,   113,    15, 35899,    21,  1412,    30,   301,    15, 35899,\n",
      "            4,   152,    16,    45,     5,   403,     4,    20,   652,    15,\n",
      "         6507,    16,    10,  8366, 37627,  5206,  1212,  1026,   373,    10,\n",
      "        10969,   102,     4,    85,    21,    45,  1412,    30, 20739,     6,\n",
      "            8,    89,    16,   117,  7407,   853,  5073,     7,  7433, 13058,\n",
      "          301, 33334,    15, 35899,     4,   345,    16,   117,  1283,    14,\n",
      "         6109,    34,   303,    14,   190,  3649,    14,    42,   652,    21,\n",
      "         1412,    30, 20739,     4,    83, 10969,   102,    16,    10,  8366,\n",
      "        37627,  5206,  3152,  9285,     6,    14,    16,   303,    15,  6507,\n",
      "            8,  3875,     4,   152,    22,  9021,   113,    15, 35899,   129,\n",
      "         1326,   101,    10,   652,   142,  5868,  3805,     7,   192,  2419,\n",
      "        11263,    52,   356,     6,  5868,    32,  3334,  2778,   592,     6,\n",
      "           61,    16,   596,    84,  2900,    16,  1887,     7,  5281,  2419,\n",
      "            4,  1876,  6556, 39726,   679,    14,  6109,    16,  9646,   301,\n",
      "           15,  6507,    31,     5,  1079,     9,     5,   232,     4,  1216,\n",
      "           82,    74,    28,   182,  1593,     4,   318,  6109,   303,   301,\n",
      "           15,  6507,     6,   172,    51,    74,   120,  2535,     9,    82,\n",
      "           18,  1503,     4,  6109,    18,  1229,    74,   712, 19167,   352,\n",
      "            6,    61,   839,    14,    49,  1138,    74,   120,  1199,    55,\n",
      "            4,   345,    16,   117,   205,  1219,    14,  6109,    74,  7433,\n",
      "          301,    15,  6507,    31,     5,  1079,     9,     5,   232,     4,\n",
      "          407,     6,  6109,    16,    45,  9646,   301,    15,  6507,    31,\n",
      "          201,     6,     8,    51,    32,    45,   667,     7,  7610,   201,\n",
      "           88,  2053,    14,     5,    22,  9021,   113,    15, 35899,    16,\n",
      "           95,    10, 10969,   102,     6,   142,    24,   888,    16,     4,\n",
      "         6109,  9646,   301,    74,    28,  4812, 32481,     6,   142,   114,\n",
      "           51,   303,   301,    15,  6507,     6,    51,    74,   146,    10,\n",
      "          319,     9,   418,     6,     8,    52,    70,   216,    14,     5,\n",
      "           82,    23,  6109,  2025,    75,  4812, 32481,    82,     4,     2,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1])\n",
      "[None, 0, 1, 2, 2, 3, 4, 5, 6, 7, 7, 7, 8, 9, 10, 11, 12, 13, 14, 15, 15, 16, 17, 18, 19, 20, 20, 21, 22, 23, 24, 25, 26, 27, 28, 28, 29, 30, 31, 32, 33, 33, 33, 34, 35, 36, 37, 38, 39, 39, 40, 41, 42, 43, 44, 44, 44, 45, 46, 47, 48, 48, 49, 50, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 68, 69, 70, 70, 71, 72, 73, 74, 74, 75, 76, 76, 77, 78, 79, 80, 81, 82, 83, 83, 84, 85, 85, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 101, 102, 103, 104, 105, 106, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 133, 134, 135, 136, 137, 138, 139, 139, 140, 141, 142, 143, 144, 145, 145, 146, 147, 148, 149, 150, 151, 152, 152, 153, 153, 154, 154, 155, 156, 157, 158, 158, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 185, 186, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 208, 208, 209, 210, 211, 212, 213, 214, 214, 214, 215, 216, 217, 218, 218, 219, 220, 221, 222, 223, 224, 224, 224, 225, 226, 227, 228, 229, 230, 231, 231, 232, 233, 234, 235, 236, 237, 238, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 248, 249, 249, 250, 250, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n"
     ]
    }
   ],
   "source": [
    "for sample in train_iter:\n",
    "    print(sample['input_ids'][0])\n",
    "    print(sample['word_ids'][0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Driverless',\n",
       " 'cars',\n",
       " 'are',\n",
       " 'exaclty',\n",
       " 'what',\n",
       " 'you',\n",
       " 'would',\n",
       " 'expect',\n",
       " 'them',\n",
       " 'to',\n",
       " 'be.',\n",
       " 'Cars',\n",
       " 'that',\n",
       " 'will',\n",
       " 'drive',\n",
       " 'without',\n",
       " 'a',\n",
       " 'person',\n",
       " 'actually',\n",
       " 'behind',\n",
       " 'the',\n",
       " 'wheel',\n",
       " 'controlling',\n",
       " 'the',\n",
       " 'actions',\n",
       " 'of',\n",
       " 'the',\n",
       " 'vehicle.',\n",
       " 'The',\n",
       " 'idea',\n",
       " 'of',\n",
       " 'driverless',\n",
       " 'cars',\n",
       " 'going',\n",
       " 'in',\n",
       " 'to',\n",
       " 'developement',\n",
       " 'shows',\n",
       " 'the',\n",
       " 'amount',\n",
       " 'of',\n",
       " 'technological',\n",
       " 'increase',\n",
       " 'that',\n",
       " 'the',\n",
       " 'wolrd',\n",
       " 'has',\n",
       " 'made.',\n",
       " 'The',\n",
       " 'leader',\n",
       " 'of',\n",
       " 'this',\n",
       " 'idea',\n",
       " 'of',\n",
       " 'driverless',\n",
       " 'cars',\n",
       " 'are',\n",
       " 'the',\n",
       " 'automobiles',\n",
       " 'they',\n",
       " 'call',\n",
       " 'Google',\n",
       " 'cars.',\n",
       " 'The',\n",
       " 'arduous',\n",
       " 'task',\n",
       " 'of',\n",
       " 'creating',\n",
       " 'safe',\n",
       " 'driverless',\n",
       " 'cars',\n",
       " 'has',\n",
       " 'not',\n",
       " 'been',\n",
       " 'fully',\n",
       " 'mastered',\n",
       " 'yet.',\n",
       " 'The',\n",
       " 'developement',\n",
       " 'of',\n",
       " 'these',\n",
       " 'cars',\n",
       " 'should',\n",
       " 'be',\n",
       " 'stopped',\n",
       " 'immediately',\n",
       " 'because',\n",
       " 'there',\n",
       " 'are',\n",
       " 'too',\n",
       " 'many',\n",
       " 'hazardous',\n",
       " 'and',\n",
       " 'dangerous',\n",
       " 'events',\n",
       " 'that',\n",
       " 'could',\n",
       " 'occur.',\n",
       " 'One',\n",
       " 'thing',\n",
       " 'that',\n",
       " 'the',\n",
       " 'article',\n",
       " 'mentions',\n",
       " 'is',\n",
       " 'that',\n",
       " 'the',\n",
       " 'driver',\n",
       " 'will',\n",
       " 'be',\n",
       " 'alerted',\n",
       " 'when',\n",
       " 'they',\n",
       " 'will',\n",
       " 'need',\n",
       " 'to',\n",
       " 'take',\n",
       " 'over',\n",
       " 'the',\n",
       " 'driving',\n",
       " 'responsibilites',\n",
       " 'of',\n",
       " 'the',\n",
       " 'car.',\n",
       " 'This',\n",
       " 'is',\n",
       " 'such',\n",
       " 'a',\n",
       " 'dangerous',\n",
       " 'thing',\n",
       " 'because',\n",
       " 'we',\n",
       " 'all',\n",
       " 'know',\n",
       " 'that',\n",
       " 'whenever',\n",
       " 'humans',\n",
       " 'get',\n",
       " 'their',\n",
       " 'attention',\n",
       " 'drawn',\n",
       " 'in',\n",
       " 'on',\n",
       " 'something',\n",
       " 'interesting',\n",
       " 'it',\n",
       " 'is',\n",
       " 'hard',\n",
       " 'to',\n",
       " 'draw',\n",
       " 'their',\n",
       " 'focus',\n",
       " 'somewhere',\n",
       " 'else.',\n",
       " 'The',\n",
       " 'article',\n",
       " 'explains',\n",
       " 'that',\n",
       " 'companies',\n",
       " 'are',\n",
       " 'trying',\n",
       " 'to',\n",
       " 'implement',\n",
       " 'vibrations',\n",
       " 'when',\n",
       " 'the',\n",
       " 'car',\n",
       " 'is',\n",
       " 'in',\n",
       " 'trouble.',\n",
       " 'Their',\n",
       " 'are',\n",
       " 'some',\n",
       " 'people',\n",
       " 'out',\n",
       " 'there',\n",
       " 'who',\n",
       " 'do',\n",
       " 'not',\n",
       " 'feel',\n",
       " 'vibrations',\n",
       " 'and',\n",
       " 'therefore',\n",
       " 'would',\n",
       " 'not',\n",
       " 'be',\n",
       " 'able',\n",
       " 'to',\n",
       " 'take',\n",
       " 'control',\n",
       " 'of',\n",
       " 'the',\n",
       " 'car',\n",
       " 'when',\n",
       " 'needed.',\n",
       " 'The',\n",
       " 'article',\n",
       " 'also',\n",
       " 'states',\n",
       " 'that',\n",
       " 'companies',\n",
       " 'are',\n",
       " 'trying',\n",
       " 'to',\n",
       " 'put',\n",
       " 'in-car',\n",
       " 'entertainment',\n",
       " 'into',\n",
       " 'the',\n",
       " 'car',\n",
       " 'while',\n",
       " 'it',\n",
       " 'is',\n",
       " 'being',\n",
       " 'driven.',\n",
       " 'This',\n",
       " 'is',\n",
       " 'just',\n",
       " 'another',\n",
       " 'thing',\n",
       " 'that',\n",
       " 'will',\n",
       " 'distract',\n",
       " 'the',\n",
       " 'person',\n",
       " 'who',\n",
       " 'is',\n",
       " 'supposed',\n",
       " 'to',\n",
       " 'be',\n",
       " 'ready',\n",
       " 'at',\n",
       " 'all',\n",
       " 'times',\n",
       " 'to',\n",
       " 'take',\n",
       " 'over',\n",
       " 'driving',\n",
       " 'when',\n",
       " 'asked',\n",
       " 'to',\n",
       " 'do',\n",
       " 'so.',\n",
       " 'Another',\n",
       " 'thing',\n",
       " 'that',\n",
       " 'can',\n",
       " 'go',\n",
       " 'wrong',\n",
       " 'with',\n",
       " 'these',\n",
       " 'cars',\n",
       " 'is',\n",
       " 'any',\n",
       " 'type',\n",
       " 'of',\n",
       " 'techological',\n",
       " 'malfucntion.',\n",
       " 'Every',\n",
       " 'person',\n",
       " 'with',\n",
       " 'any',\n",
       " 'kind',\n",
       " 'of',\n",
       " 'technological',\n",
       " 'device',\n",
       " 'has',\n",
       " 'experienced',\n",
       " 'some',\n",
       " 'sort',\n",
       " 'of',\n",
       " 'error.',\n",
       " 'Now',\n",
       " 'imagine',\n",
       " 'if',\n",
       " 'your',\n",
       " 'car',\n",
       " 'has',\n",
       " 'an',\n",
       " 'error',\n",
       " 'technologically',\n",
       " 'and',\n",
       " 'it',\n",
       " 'takes',\n",
       " 'the',\n",
       " 'life',\n",
       " 'of',\n",
       " 'one',\n",
       " 'your',\n",
       " 'loved',\n",
       " 'ones.',\n",
       " 'The',\n",
       " 'article',\n",
       " 'talks',\n",
       " 'about',\n",
       " 'sensors',\n",
       " 'around',\n",
       " 'the',\n",
       " 'car',\n",
       " 'that',\n",
       " 'read',\n",
       " 'the',\n",
       " 'surroundings',\n",
       " 'of',\n",
       " 'the',\n",
       " 'car',\n",
       " 'and',\n",
       " 'that',\n",
       " 'is',\n",
       " 'what',\n",
       " 'helps',\n",
       " 'he',\n",
       " 'car',\n",
       " 'to',\n",
       " 'drive',\n",
       " 'without',\n",
       " 'a',\n",
       " 'true',\n",
       " 'driver',\n",
       " 'behind',\n",
       " 'the',\n",
       " 'wheel.',\n",
       " 'Those',\n",
       " 'sensors',\n",
       " 'could',\n",
       " 'have',\n",
       " 'a',\n",
       " 'malfunctions',\n",
       " 'and',\n",
       " 'be',\n",
       " 'sensing',\n",
       " 'something',\n",
       " 'that',\n",
       " 'is',\n",
       " 'that',\n",
       " 'even',\n",
       " 'there',\n",
       " 'and',\n",
       " 'make',\n",
       " 'a',\n",
       " 'left',\n",
       " 'turn',\n",
       " 'into',\n",
       " 'a',\n",
       " '100',\n",
       " 'foot',\n",
       " 'deep',\n",
       " 'lake.',\n",
       " 'The',\n",
       " 'vibrations',\n",
       " 'that',\n",
       " 'cause',\n",
       " 'the',\n",
       " 'driver',\n",
       " 'to',\n",
       " 'be',\n",
       " 'notified',\n",
       " 'to',\n",
       " 'drive',\n",
       " 'could',\n",
       " 'malfunction',\n",
       " 'and',\n",
       " 'now',\n",
       " 'the',\n",
       " 'driver',\n",
       " 'has',\n",
       " 'no',\n",
       " 'way',\n",
       " 'of',\n",
       " 'knowing',\n",
       " 'that',\n",
       " 'the',\n",
       " 'car',\n",
       " 'is',\n",
       " 'in',\n",
       " 'trouble',\n",
       " 'and',\n",
       " 'now',\n",
       " 'you,',\n",
       " 'the',\n",
       " 'driver,',\n",
       " 'and',\n",
       " 'the',\n",
       " 'rest',\n",
       " 'of',\n",
       " 'your',\n",
       " 'passengers',\n",
       " 'are',\n",
       " 'being',\n",
       " 'buried',\n",
       " 'in',\n",
       " 'your',\n",
       " 'local',\n",
       " 'cemetery.',\n",
       " 'One',\n",
       " 'last',\n",
       " 'thing',\n",
       " 'that',\n",
       " 'the',\n",
       " 'article',\n",
       " 'mentions',\n",
       " 'is',\n",
       " 'negative',\n",
       " 'about',\n",
       " 'the',\n",
       " 'developement',\n",
       " 'of',\n",
       " 'driverless',\n",
       " 'cars',\n",
       " 'is',\n",
       " 'who',\n",
       " 'to',\n",
       " 'blame',\n",
       " 'for',\n",
       " 'the',\n",
       " 'wreck',\n",
       " 'if',\n",
       " 'there',\n",
       " 'were',\n",
       " 'possibly',\n",
       " 'some',\n",
       " 'sort',\n",
       " 'of',\n",
       " 'technological',\n",
       " 'malfunciton',\n",
       " 'or',\n",
       " 'even',\n",
       " 'some',\n",
       " 'sort',\n",
       " 'of',\n",
       " 'human',\n",
       " 'error',\n",
       " 'when',\n",
       " 'taking',\n",
       " 'over',\n",
       " 'the',\n",
       " 'driving',\n",
       " 'aspect.',\n",
       " 'Should',\n",
       " 'the',\n",
       " 'manufacturer',\n",
       " 'of',\n",
       " 'the',\n",
       " 'car',\n",
       " 'be',\n",
       " 'blamed',\n",
       " 'or',\n",
       " 'should',\n",
       " 'it',\n",
       " 'be',\n",
       " 'the',\n",
       " 'driver?',\n",
       " 'No',\n",
       " 'one',\n",
       " 'knows',\n",
       " 'because',\n",
       " 'there',\n",
       " 'is',\n",
       " 'so',\n",
       " 'many',\n",
       " 'different',\n",
       " 'factors',\n",
       " 'that',\n",
       " 'attribute',\n",
       " 'to',\n",
       " 'who',\n",
       " 'to',\n",
       " 'assign',\n",
       " 'the',\n",
       " 'blame',\n",
       " 'to.',\n",
       " 'Some',\n",
       " 'of',\n",
       " 'what',\n",
       " 'will',\n",
       " 'have',\n",
       " 'to',\n",
       " 'be',\n",
       " 'made',\n",
       " 'is',\n",
       " 'a',\n",
       " 'judgement',\n",
       " 'call.',\n",
       " 'When',\n",
       " 'it',\n",
       " 'comes',\n",
       " 'to',\n",
       " 'insurance',\n",
       " 'and',\n",
       " 'having',\n",
       " 'to',\n",
       " 'pay',\n",
       " 'for',\n",
       " 'any',\n",
       " 'damages',\n",
       " 'you',\n",
       " 'do',\n",
       " 'not',\n",
       " 'want',\n",
       " 'someone',\n",
       " 'to',\n",
       " 'have',\n",
       " 'to',\n",
       " 'make',\n",
       " 'some',\n",
       " 'sort',\n",
       " 'of',\n",
       " 'judgement',\n",
       " 'call.',\n",
       " 'What',\n",
       " 'if',\n",
       " 'that',\n",
       " 'judgement',\n",
       " 'call',\n",
       " 'that',\n",
       " 'was',\n",
       " 'made',\n",
       " 'was',\n",
       " 'the',\n",
       " 'wrong',\n",
       " 'call?',\n",
       " 'Now',\n",
       " 'there',\n",
       " 'are',\n",
       " 'going',\n",
       " 'to',\n",
       " 'be',\n",
       " 'even',\n",
       " 'more',\n",
       " 'lawsuits',\n",
       " 'today',\n",
       " 'in',\n",
       " 'our',\n",
       " 'courts',\n",
       " 'than',\n",
       " 'there',\n",
       " 'already',\n",
       " 'are.',\n",
       " 'This',\n",
       " 'problem',\n",
       " 'alone',\n",
       " 'will',\n",
       " 'just',\n",
       " 'lead',\n",
       " 'to',\n",
       " 'many',\n",
       " 'more',\n",
       " 'issues',\n",
       " 'today',\n",
       " 'in',\n",
       " 'the',\n",
       " 'world',\n",
       " 'that',\n",
       " 'should',\n",
       " 'not',\n",
       " 'have',\n",
       " 'to',\n",
       " 'be',\n",
       " 'dealt',\n",
       " 'with.',\n",
       " 'With',\n",
       " 'all',\n",
       " 'these',\n",
       " 'things',\n",
       " 'that',\n",
       " 'could',\n",
       " 'possibly',\n",
       " 'go',\n",
       " 'wrong',\n",
       " 'with',\n",
       " 'these',\n",
       " 'driverless',\n",
       " 'cars',\n",
       " 'there',\n",
       " 'is',\n",
       " 'no',\n",
       " 'way',\n",
       " 'that',\n",
       " 'the',\n",
       " 'developement',\n",
       " 'of',\n",
       " 'them',\n",
       " 'should',\n",
       " 'continue',\n",
       " 'any',\n",
       " 'further.',\n",
       " 'In',\n",
       " \"today's\",\n",
       " 'society',\n",
       " 'if',\n",
       " 'something',\n",
       " 'bad',\n",
       " 'COULD',\n",
       " 'happen',\n",
       " 'or',\n",
       " 'something',\n",
       " 'COULD',\n",
       " 'go',\n",
       " 'wrong,',\n",
       " 'it',\n",
       " 'WILL',\n",
       " 'happen,',\n",
       " 'and',\n",
       " 'it',\n",
       " 'WILL',\n",
       " 'go',\n",
       " 'wrong.',\n",
       " 'There',\n",
       " 'are',\n",
       " 'just',\n",
       " 'way',\n",
       " 'too',\n",
       " 'many',\n",
       " 'safety',\n",
       " 'hazards',\n",
       " 'that',\n",
       " 'come',\n",
       " 'along',\n",
       " 'with',\n",
       " 'these',\n",
       " 'driverless',\n",
       " 'cars.',\n",
       " 'Becuase',\n",
       " 'of',\n",
       " 'all',\n",
       " 'of',\n",
       " 'these',\n",
       " 'problems',\n",
       " 'that',\n",
       " 'arise',\n",
       " 'with',\n",
       " 'the',\n",
       " 'cars',\n",
       " 'it',\n",
       " 'is',\n",
       " 'just',\n",
       " 'a',\n",
       " 'gargantuan',\n",
       " 'risk',\n",
       " 'to',\n",
       " 'implement',\n",
       " 'these',\n",
       " 'cars',\n",
       " 'into',\n",
       " 'our',\n",
       " 'lifestyles.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_ids = sample['word_ids'][1]\n",
    "train_dataset[1]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForTokenClassification: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(config['model'], num_labels=15).to(device)\n",
    "\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\"params\": [p for n, p in model.named_parameters()\n",
    "                if not any(nd in n for nd in no_decay)],\n",
    "     \"weight_decay\": config['weight_decay'],\n",
    "     },\n",
    "    {\"params\": [p for n, p in model.named_parameters()\n",
    "                if any(nd in n for nd in no_decay)],\n",
    "     \"weight_decay\": 0.0,\n",
    "     },\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                  lr=config['lr'],\n",
    "                  betas=(0.9, 0.999),\n",
    "                  eps=1e-6\n",
    "                  )\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=config['lr_scheduler_type'],\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=config['num_warmup_steps'],\n",
    "    num_training_steps=config['epochs'] * len(train_iter) /  config['gradient_accumulation_steps'], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1950/1950 [13:29<00:00,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1  Average Training Loss:0.914627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "words, losses = [], []\n",
    "tk = tqdm(train_iter, total=len(train_iter), position=0, leave=True)\n",
    "model.train()\n",
    "step = 0\n",
    "for epoch in range(config['epochs']):\n",
    "    print(\"Epoch {}/{}\".format(epoch, config['epochs']))\n",
    "    for batch in tk:\n",
    "        word_ids = batch['word_ids']\n",
    "        words.extend(word_ids)\n",
    "        batch = {k: v.to(device) for k, v in batch.items() if k != 'word_ids'}\n",
    "        loss = model(input_ids=batch['input_ids'],\n",
    "                     attention_mask=batch['attention_mask'],\n",
    "                     labels=batch['labels']).loss\n",
    "        loss /= config['gradient_accumulation_steps']\n",
    "        loss.backward()\n",
    "        if (step + 1) % config['gradient_accumulation_steps'] == 0:\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        step += 1\n",
    "        losses.append(loss.item() * config['gradient_accumulation_steps'])\n",
    "    # print average loss\n",
    "    print(\"Epoch {}/{}  Average Training Loss:{:6f}\".format(\n",
    "        epoch,\n",
    "        config['epochs'],\n",
    "        np.mean(losses)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>class</th>\n",
       "      <th>predictionstring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18409261F5C2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D46BCB48440A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0FB0700DAF44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D72CB1C11673</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DF920E0A7337</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  class  predictionstring\n",
       "0  18409261F5C2    NaN               NaN\n",
       "1  D46BCB48440A    NaN               NaN\n",
       "2  0FB0700DAF44    NaN               NaN\n",
       "3  D72CB1C11673    NaN               NaN\n",
       "4  DF920E0A7337    NaN               NaN"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('./data/sample_submission.csv')\n",
    "test_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 4356.36it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0FB0700DAF44</td>\n",
       "      <td>[During, a, group, project,, have, you, ever, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18409261F5C2</td>\n",
       "      <td>[80%, of, Americans, believe, seeking, multipl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D46BCB48440A</td>\n",
       "      <td>[When, people, ask, for, advice,they, sometime...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D72CB1C11673</td>\n",
       "      <td>[Making, choices, in, life, can, be, very, dif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DF920E0A7337</td>\n",
       "      <td>[Have, you, ever, asked, more, than, one, pers...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                               text\n",
       "0  0FB0700DAF44  [During, a, group, project,, have, you, ever, ...\n",
       "1  18409261F5C2  [80%, of, Americans, believe, seeking, multipl...\n",
       "2  D46BCB48440A  [When, people, ask, for, advice,they, sometime...\n",
       "3  D72CB1C11673  [Making, choices, in, life, can, be, very, dif...\n",
       "4  DF920E0A7337  [Have, you, ever, asked, more, than, one, pers..."
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_names, test_texts = [], []\n",
    "for f in tqdm(list(os.listdir('./data/test'))):\n",
    "    test_names.append(f.replace('.txt', ''))\n",
    "    with open('./data/test/' + f, 'r', encoding='utf-8') as f:\n",
    "        text = ''\n",
    "        for line in f.readlines():\n",
    "            #text += line.replace('\\n', '').replace('\\xa0', '')\n",
    "            text += line.replace('\\n', ' ')\n",
    "        test_texts.append(text)\n",
    "test_texts = pd.DataFrame({'id': test_names, 'text': test_texts})\n",
    "test_texts['text'] = test_texts['text'].apply(lambda x: x.split())\n",
    "test_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = MyDataset(test_texts, phase='Test')\n",
    "test_iter = DataLoader(test_dataset, batch_size=config['valid_bs'], collate_fn=collate_fn, shuffle=False,\n",
    "                       num_workers=config['num_workers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  7.72it/s]\n"
     ]
    }
   ],
   "source": [
    "y_pred = []\n",
    "words = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    tk = tqdm(test_iter, total=len(test_iter), position=0, leave=True)\n",
    "    for step, batch in enumerate(tk):\n",
    "        word_ids = batch['word_ids']\n",
    "        words.extend(word_ids)\n",
    "        batch = {k: v.to(device) for k, v in batch.items() if k != 'word_ids'}\n",
    "\n",
    "        output = model(input_ids=batch['input_ids'],\n",
    "                       attention_mask=batch['attention_mask']).logits\n",
    "\n",
    "        y_pred.extend(output.argmax(-1).cpu().numpy())\n",
    "\n",
    "y_pred = np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(test_texts))):\n",
    "    idx = test_texts.id.values[i]\n",
    "    pred = [''] * len(test_texts.text.values[i])\n",
    "\n",
    "    for j in range(len(y_pred[i])):\n",
    "        if words[i][j] != None:\n",
    "            pred[words[i][j]] = labels[y_pred[i][j]]\n",
    "\n",
    "    pred = [x.replace('B-', '').replace('I-', '') for x in pred]\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lead',\n",
       " 'Lead',\n",
       " 'Lead',\n",
       " 'Lead',\n",
       " 'Lead',\n",
       " 'Lead',\n",
       " 'Lead',\n",
       " 'Lead',\n",
       " 'Lead',\n",
       " 'Lead',\n",
       " 'Lead',\n",
       " 'Lead',\n",
       " 'Lead',\n",
       " 'Lead',\n",
       " 'Lead',\n",
       " 'Lead',\n",
       " 'Lead',\n",
       " 'Lead',\n",
       " 'Lead',\n",
       " 'Lead',\n",
       " 'Lead',\n",
       " 'Lead',\n",
       " 'Lead',\n",
       " 'Lead',\n",
       " 'Lead',\n",
       " 'Lead',\n",
       " 'Lead',\n",
       " 'Lead',\n",
       " 'Lead',\n",
       " 'Lead',\n",
       " 'Lead',\n",
       " 'Lead',\n",
       " 'Lead',\n",
       " 'Lead',\n",
       " 'Lead',\n",
       " 'Lead',\n",
       " 'Lead',\n",
       " 'Position',\n",
       " 'Position',\n",
       " 'Position',\n",
       " 'Position',\n",
       " 'Position',\n",
       " 'Position',\n",
       " 'Position',\n",
       " 'Position',\n",
       " 'Position',\n",
       " 'Position',\n",
       " 'Position',\n",
       " 'Claim',\n",
       " 'Claim',\n",
       " 'Claim',\n",
       " 'Claim',\n",
       " 'Claim',\n",
       " 'Position',\n",
       " 'Claim',\n",
       " 'Claim',\n",
       " 'Claim',\n",
       " 'Claim',\n",
       " 'Claim',\n",
       " 'Claim',\n",
       " 'Claim',\n",
       " 'Claim',\n",
       " 'Claim',\n",
       " 'Claim',\n",
       " 'Claim',\n",
       " 'Claim',\n",
       " 'Claim',\n",
       " 'Claim',\n",
       " 'Claim',\n",
       " 'Claim',\n",
       " 'Claim',\n",
       " 'Claim',\n",
       " 'Claim',\n",
       " 'Claim',\n",
       " 'Claim',\n",
       " 'Claim',\n",
       " 'Claim',\n",
       " 'Claim',\n",
       " 'Claim',\n",
       " 'Claim',\n",
       " 'Claim',\n",
       " 'Claim',\n",
       " 'Claim',\n",
       " 'Evidence',\n",
       " 'Claim',\n",
       " 'Claim',\n",
       " 'Claim',\n",
       " 'Claim',\n",
       " 'Claim',\n",
       " 'Claim',\n",
       " 'Claim',\n",
       " 'Claim',\n",
       " 'Claim',\n",
       " 'Claim',\n",
       " 'Claim',\n",
       " 'Claim',\n",
       " 'Claim',\n",
       " 'Claim',\n",
       " 'Claim',\n",
       " 'Claim',\n",
       " 'Claim',\n",
       " 'Claim',\n",
       " 'Claim',\n",
       " 'Claim',\n",
       " 'Claim',\n",
       " 'Claim',\n",
       " 'Claim',\n",
       " 'O',\n",
       " 'Claim',\n",
       " 'O',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Claim',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " 'Evidence',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "final_preds = []\n",
    "\n",
    "for i in tqdm(range(len(test_texts))):\n",
    "    idx = test_texts.id.values[i]\n",
    "    pred = [''] * len(test_texts.text.values[i])\n",
    "\n",
    "    for j in range(len(y_pred[i])):\n",
    "        if words[i][j] != None:\n",
    "            pred[words[i][j]] = labels[y_pred[i][j]]\n",
    "\n",
    "    pred = [x.replace('B-', '').replace('I-', '') for x in pred]\n",
    "\n",
    "    preds = []\n",
    "    j = 0\n",
    "    while j < len(pred):\n",
    "        cls = pred[j]\n",
    "        if cls == 'O':\n",
    "            j += 1\n",
    "        end = j + 1\n",
    "        while end < len(pred) and pred[end] == cls:\n",
    "            end += 1\n",
    "\n",
    "        if cls != 'O' and cls != '' and end - j > 10:\n",
    "            final_preds.append((idx, cls, ' '.join(map(str, list(range(j, end))))))\n",
    "        \n",
    "        j = end\n",
    "\n",
    "final_preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sub = pd.DataFrame(final_preds)\n",
    "sub.columns = test_df.columns\n",
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sub.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
